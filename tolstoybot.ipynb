{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Model/layer imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #If using GPU, reset memory allocation, and assign GPU with memory growth.\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open War and Peace\n",
    "path_to_file = \"tolstoybot.txt\"\n",
    "text = open(path_to_file, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain unique vocab set and size \n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index mapping for characters in vocab (bi-directional)\n",
    "char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode whole of text file using vocab index map\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Query line(s) length(s)\n",
    "\n",
    "# line = 'Well, Prince, so Genoa and Lucca are now just family estates of the Buonapartes.'\n",
    "# print(len(line))\n",
    "\n",
    "# lines='''\n",
    "# But I warn you, if you don’t tell me that this means war,\n",
    "# if you still try to defend the infamies and horrors perpetrated by that\n",
    "# Antichrist—I really believe he is Antichrist—I will have nothing\n",
    "# more to do with you and you are no longer my friend, no longer my\n",
    "# ‘faithful slave,’ as you call yourself!\n",
    "#   '''\n",
    "# print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set length of sequence\n",
    "seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine total number of sequences in text\n",
    "total_num_seq = len(text) // (seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Character Dataset from encoded text \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine into sequences\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method for splitting sequences into input/target \n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "\n",
    "#Use method to create complete training dataset\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set batch size\n",
    "batch_size = 32\n",
    "#Set buffer size\n",
    "buffer_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set embedding dimensions\n",
    "embed_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose amount of neurons in GRU layers\n",
    "rnn_neurons4 = 1024\n",
    "rnn_neurons3 = 512\n",
    "rnn_neurons2 = 256\n",
    "rnn_neurons = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customised Sparse_cat_cross loss function \n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating model\n",
    "def create_model(vocab_size, embed_dim,rnn_neurons,rnn_neurons2,rnn_neurons3,rnn_neurons4,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size,None]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform', batch_input_shape=[batch_size,None, embed_dim]))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons2, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons3, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons4, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile('adam', loss=sparse_cat_loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training model\n",
    "model = create_model(vocab_size=vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     rnn_neurons2=rnn_neurons2,\n",
    "                     rnn_neurons3=rnn_neurons3,\n",
    "                     rnn_neurons4=rnn_neurons4,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           26880     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (32, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (32, None, 105)           114345    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (32, None, 105)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (32, None, 256)           278784    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (32, None, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (32, None, 512)           1182720   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (32, None, 512)           0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (32, None, 1024)          4724736   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (32, None, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 105)           107625    \n",
      "=================================================================\n",
      "Total params: 6,435,090\n",
      "Trainable params: 6,435,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set epochs \n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create early stopping function\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 2.0598 - accuracy: 0.4176\n",
      "Epoch 2/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.4319 - accuracy: 0.5734\n",
      "Epoch 3/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.3396 - accuracy: 0.5968\n",
      "Epoch 4/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.2968 - accuracy: 0.6075\n",
      "Epoch 5/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.2715 - accuracy: 0.6147\n",
      "Epoch 6/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.2539 - accuracy: 0.6189\n",
      "Epoch 7/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.2417 - accuracy: 0.6225\n",
      "Epoch 8/100\n",
      "775/775 [==============================] - 43s 56ms/step - loss: 1.2337 - accuracy: 0.6246\n",
      "Epoch 9/100\n",
      "775/775 [==============================] - 44s 56ms/step - loss: 1.2274 - accuracy: 0.6263\n",
      "Epoch 10/100\n",
      "775/775 [==============================] - 44s 56ms/step - loss: 1.2233 - accuracy: 0.6272\n",
      "Epoch 11/100\n",
      "775/775 [==============================] - 44s 56ms/step - loss: 1.2202 - accuracy: 0.6281\n",
      "Epoch 12/100\n",
      "775/775 [==============================] - 44s 56ms/step - loss: 1.2190 - accuracy: 0.6283\n",
      "Epoch 13/100\n",
      "775/775 [==============================] - 44s 56ms/step - loss: 1.2184 - accuracy: 0.6284\n",
      "Epoch 14/100\n",
      "775/775 [==============================] - 44s 57ms/step - loss: 1.2192 - accuracy: 0.6284\n",
      "Epoch 15/100\n",
      "775/775 [==============================] - 44s 56ms/step - loss: 1.2202 - accuracy: 0.6280\n",
      "Epoch 16/100\n",
      "775/775 [==============================] - 44s 57ms/step - loss: 1.2223 - accuracy: 0.6277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26d3b835e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(dataset,epochs=epochs, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save('tolstoybot_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new version of the model (test_model) with a single input batch size\n",
    "test_model = create_model(vocab_size,embed_dim,rnn_neurons, rnn_neurons2, rnn_neurons3, rnn_neurons4, batch_size=1)\n",
    "test_model.load_weights('tolstoybot_train.h5')\n",
    "test_model.build(tf.TensorShape([1,None]))\n",
    "test_model.save('tolstoybot_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE BELOW CODE IF YOU ARE OPENING THIS AFTER TRAINING###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If loading seperately\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # #if using GPU, set the correct GPU and assign memory growth\n",
    "# # tf.compat.v1.reset_default_graph()\n",
    "# # physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# # tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "# #Open tolstoybot.txt\n",
    "# path_to_file = \"tolstoybot.txt\"\n",
    "# text = open(path_to_file, 'r', encoding='utf-8').read()\n",
    "\n",
    "# #Obtain unique vocab set and size \n",
    "# vocab = sorted(set(text))\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# #Create an index mapping for characters in vocab (bi-directional)\n",
    "# char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "# ind_to_char = np.array(vocab)\n",
    "\n",
    "# #Load the model to memory\n",
    "# test_model = load_model('tolstoybot_test.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE ABOVE CODE IF YOU ARE OPENING THIS AFTER TRAINING###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            26880     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 105)            114345    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, None, 105)            0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 256)            278784    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (1, None, 512)            1182720   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (1, None, 512)            0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, None, 1024)           4724736   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 105)            107625    \n",
      "=================================================================\n",
      "Total params: 6,435,090\n",
      "Trainable params: 6,435,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display test model summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test function for generating consecutive text\n",
    "def generate_text(model,start_seed,num_generate=500,temperature=1):\n",
    "    \n",
    "    #Map each character in start_seed to it's relative index\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    \n",
    "    #Expand the dimensions\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    #Create empty list for generated text\n",
    "    text_generated = []\n",
    "    \n",
    "    #Reset the states of tyhe model        \n",
    "    model.reset_states()\n",
    "    \n",
    "    #for each iteration of num_generate\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        #Obtain probability matrix for current iteration \n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        #Reduce dimensions\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        #Multiply probabily matrix by temperature\n",
    "        predictions = predictions/temperature\n",
    "        \n",
    "        #Select a random outcome, based on the unnormalised log-probabilities produced by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #Expand dimensions of prediction and assign as next input evaluation\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        #Convert prediction to char and append to generated list of text\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "        \n",
    "    #return the initial input, concatenated with the generated text. \n",
    "    return(start_seed+\"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Temperature affects the probability of characters chosen, should range between 0.1 and 2\n",
    "# Note: A smaller temperature will result in more predictable text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test of history of the whole house the\n",
      "meaning of the action of the highest defense of the army was described to him.\n",
      "\n",
      "“What a men are the regimental service at the French army to everything and to the influence of the world. I don’t know why she has\n",
      "been saying about the whole affair to do so.\n",
      "\n",
      "“I have to be a suble general saying.”\n",
      "\n",
      "“Well, then has been in the forest and so that it is all that was the best of this better to let them the same time the house in the freedom and saw\n",
      "that they had been\n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample text \n",
    "print(generate_text(test_model,\"This is a test\", num_generate=500, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
