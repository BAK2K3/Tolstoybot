{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Model/layer imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#If using GPU, reset memory allocation, and assign GPU with memory growth.\n",
    "tf.compat.v1.reset_default_graph()\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open War and Peace\n",
    "path_to_file = \"tolstoybot.txt\"\n",
    "text = open(path_to_file, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain unique vocab set and size \n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index mapping for characters in vocab (bi-directional)\n",
    "char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode whole of text file using vocab index map\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Query line(s) length(s)\n",
    "\n",
    "# line = 'Well, Prince, so Genoa and Lucca are now just family estates of the Buonapartes.'\n",
    "# print(len(line))\n",
    "\n",
    "# lines='''\n",
    "# But I warn you, if you don’t tell me that this means war,\n",
    "# if you still try to defend the infamies and horrors perpetrated by that\n",
    "# Antichrist—I really believe he is Antichrist—I will have nothing\n",
    "# more to do with you and you are no longer my friend, no longer my\n",
    "# ‘faithful slave,’ as you call yourself!\n",
    "#   '''\n",
    "# print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set length of sequence\n",
    "seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine total number of sequences in text\n",
    "total_num_seq = len(text) // (seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Character Dataset from encoded text \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine into sequences\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method for splitting sequences into input/target \n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "\n",
    "#Use method to create complete training dataset\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set batch size\n",
    "batch_size = 32\n",
    "#Set buffer size\n",
    "buffer_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #view shape of dataset\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set embedding dimensions\n",
    "embed_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose amount of neurons in GRU layers\n",
    "rnn_neurons4 = 1024\n",
    "rnn_neurons3 = 512\n",
    "rnn_neurons2 = 256\n",
    "rnn_neurons = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customised Sparse_cat_cross loss function \n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating model\n",
    "def create_model(vocab_size, embed_dim,rnn_neurons,rnn_neurons2,rnn_neurons3,rnn_neurons4,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size,None]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform', batch_input_shape=[batch_size,None, embed_dim]))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons2, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons3, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(rnn_neurons4, return_sequences=True, \n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile('adam', loss=sparse_cat_loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training model\n",
    "model = create_model(vocab_size=vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     rnn_neurons2=rnn_neurons2,\n",
    "                     rnn_neurons3=rnn_neurons3,\n",
    "                     rnn_neurons4=rnn_neurons4,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set epochs \n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create early stopping function\n",
    "early_stop = EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "model.fit(dataset,epochs=epochs, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save('tolstoybot_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new version of the model (test_model) with a single input batch size\n",
    "test_model = create_model(vocab_size,embed_dim,rnn_neurons, rnn_neurons2, rnn_neurons3, rnn_neurons4, batch_size=1)\n",
    "test_model.load_weights('tolstoybot_train.h5')\n",
    "test_model.build(tf.TensorShape([1,None]))\n",
    "test_model.save('tolstoybot_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE BELOW CODE IF YOU ARE OPENING THIS AFTER TRAINING###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If loading seperately\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # #if using GPU, set the correct GPU and assign memory growth\n",
    "# # tf.compat.v1.reset_default_graph()\n",
    "# # physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# # tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "# #Open tolstoybot.txt\n",
    "# path_to_file = \"tolstoybot.txt\"\n",
    "# text = open(path_to_file, 'r', encoding='utf-8').read()\n",
    "\n",
    "# #Obtain unique vocab set and size \n",
    "# vocab = sorted(set(text))\n",
    "# vocab_size = len(vocab)\n",
    "\n",
    "# #Create an index mapping for characters in vocab (bi-directional)\n",
    "# char_to_ind = {char:ind for ind, char in enumerate(vocab)}\n",
    "# ind_to_char = np.array(vocab)\n",
    "\n",
    "# #Load the model to memory\n",
    "# test_model = load_model('tolstoybot_test.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN THE ABOVE CODE IF YOU ARE OPENING THIS AFTER TRAINING###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            26880     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 105)            114345    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (1, None, 105)            0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 256)            278784    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (1, None, 512)            1182720   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (1, None, 512)            0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, None, 1024)           4724736   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 105)            107625    \n",
      "=================================================================\n",
      "Total params: 6,435,090\n",
      "Trainable params: 6,435,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display test model summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test function for generating consecutive text\n",
    "def generate_text(model,start_seed,num_generate=500,temperature=1):\n",
    "    \n",
    "    #Map each character in start_seed to it's relative index\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    \n",
    "    #Expand the dimensions\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    #Create empty list for generated text\n",
    "    text_generated = []\n",
    "    \n",
    "    #Reset the states of tyhe model        \n",
    "    model.reset_states()\n",
    "    \n",
    "    #for each iteration of num_generate\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        #Obtain probability matrix for current iteration \n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        #Reduce dimensions\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        #Multiply probabily matrix by temperature\n",
    "        predictions = predictions/temperature\n",
    "        \n",
    "        #Select a random outcome, based on the unnormalised log-probabilities produced by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #Expand dimensions of prediction and assign as next input evaluation\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        #Convert prediction to char and append to generated list of text\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "        \n",
    "    #return the initial input, concatenated with the generated text. \n",
    "    return(start_seed+\"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Temperature affects the probability of characters chosen, should range between 0.1 and 2\n",
    "# Note: A smaller temperature will result in more predictable text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a testain pretending to ask him to say that he had reached the\n",
      "Emperor who was the soldier of the people with the movement of the conception of\n",
      "the country. The last days were being led him and go to the cause of the\n",
      "army, and the count’s hussar and such full of strength and considerations and\n",
      "the people are essential to the Emperor and have been in the same man days in the\n",
      "conception of the character in the army because they have been\n",
      "received and she went out of the way had been in the commander in \n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample text \n",
    "print(generate_text(test_model,\"This is a test\", num_generate=500, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
